## PROJECT WebPage-Classifier-Engine

[Introduction](#introduction)
1. [Port Scanning](#port-scanning)
2. [Web Page donwloading](#web-page-downloading)
3. [Web Page Classifier](#web-page-classifier)
	- [Headers](#headers)
	- [HTML](#html)
	- [Text](#text)
3. [Web Page Analyzer](#web-page-analyzer)
	- [Screenshot](#screenshot)
	
	

### Introduction
The main aim of WebPage-Classifier-Engine is to be able to access, identify, and evaluate the.onion or clearnet web pages based on the keywords provided by the user. In addition, web pages are also evaluated based on their HTML properties such as the `HTML to text ratio`, the existence of certain `HTML headers` in the HTTP response of the website. The program applies request-library to fetch webpages and Pysocks library to interact with the Tor Linux library. For the classification of webpages, Spacy (Natural Language Processing) was employed.

### Installation
Download the tor library from your respective linux repository

```bash
sudo apt-get install tor
```

It would be wise to first set up a virtual environment and then install the requirements.txt. Once the `Spacy=2.2.4` has been installed, the spacy English language models must be installed `en_core_web_lg` & `en_core_web_sm`. (The essential language models are downloaded only after the requirements.txt has been installed, since the `Spacy=2.2.4` is required to run the project.)

```bash
python -m spacy en_core_web_lg
```

```bash
python -m spacy en_core_web_sm
```

### Usage
The program can be run using the bash script `run.sh`.

```bash
bash run.sh
```

The script will run [WebPageDownloader.py](WebPageDownloader.py), which will prompt for a file path containing a list of URLs. The package contains a folder called [`Key_List`](Key_List) containing five text files. Files are used to specify keywords, if found in the html content, will be used to calculate the ranking of the pages.

**Example :** [emails_list](Key_List/emails_list) has the keyword 'imam@gmail.com'. The program [webpageclassifier.py](webpageclassifier.py) when classifying the webpage will look
for this particular email-id. Considering the find, the program will assign some more weight to the webpage the emailId was found in, in comparisson to the others it didn't find it in. Same goes with the other lists. 

The full working can be understood below.

### Port Scanning
Port scanning is performed via Pysocks library and only over selected ports, which is why port numbers are hard-coded port in the program itself. This is because tor has a protective mechanism in place that detects port scanning whenever an unrecognized port is accessed, thus considering the above-mentioned condition a time delay of one second was introduced while coding the part of the scanner and no threading was employed. This is based on the [(2019)research paper's](https://dl.acm.org/doi/pdf/10.1145/3339252.3341486?download=true) finding. The port numbers list will have to set manually by editing the code [portScanner.py](portScanner.py).

The ports identified running any webhosting scheme (http/https:) are pused to requests python library, which hits the end point and later processed by other functions.

The port numbers selected on the basis of this [(2014)study](https://arxiv.org/pdf/1308.6768.pdf).

### Web Page donwloading
This being the parent script of all the scripts is called first when executing the `run.sh`. The websites are downloaded in the hierarchy as follows.

```
{DATE}_hour
    |
    |---HTML
        |
        |--{urlname}.html..
    |---Headers
        |
        |--{urlname}_headers.json..
    |---Images
        |
        |--{urlname}.png..
    |---downloaded.json
    |---final_1.json
    |---report.html
```

The `downloaded.json` (intial json downloads indexing which webpages were downloaded, redundancy file incase the classifier is needed to be rerun) is generated by [WebPageDownloader.py](WebPageDownloader.py), `final_1.json`( is the final json file, used to generate report) by the [webpageclassifier.py](webpageclassifier.py) and `report.html` by [webPageAnalyzer.py](webPageAnalyzer.py).

The keys of the `downloaded.json` are subset of `final_1.json`. The keys(self-explanatory) of `final_1.json` are:

```json
[{"url": "", "port": "", "is_redirect": True , "html_response": , "html_path": "2020-06-26_00/HTML/https:__{url}l", "headers_path": "2020-06-26_00/Headers/https:__{url}_header.json", "image_path": "2020-06-26_00/Images/https:__{url}.png", "script": false, "interest": 1.27, "location_list": [], "person_list": [], "dates_list": [], "organisation_list": [], "emails_list": [], "combo_basic": []}]
```

### Web Page Classifier

The core of the program, classifies the webpage based on three parameters, information reveal relevance (side-channels) and user provided parameters. The process is heavy and hence multi-processing was used, thus making the process a little faster, but might heavy on use. 

#### Headers

```python 
def test_on_headers(path_header):
```

The HTTP response from any website is ranked on the basis of presence of these headers, hardcoded in the code [webpageclassifier.py](webpageclassifier.py) in an array.

```python
    check_headers = ['Server', 'Content-Type', 'Last-Modified', 'Set-Cookie', 'WWW-Authenticate', 'Alt-Svc',
                     'Content-Disposition', 'Content-Security-Policy', 'Strict-Transport-Security', 'ETag']
```
The relevancy of selecting each header can be read on [MDN web records](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers). Based on these, which can be changed a simple ratio calculation is performed to determine the score the webpage must be provided with.

#### HTML 

```python 
def test_on_html(html_path):
```

The HTML score of the web page is calculated on the tags [feature extaction paper](http://ijcsit.com/docs/Volume%207/vol7issue2/ijcsit2016070218.pdf). Namely the HTML tags like `<img> <script> <input>` are checked to determine if a webpage is a forum or a login page. This also, handles if a page throws a 404 response or 303. The other feature on which the ranking is determined is the `HTML:TEXT (ratio)`. Together all these constitute to points upto (2.0). Its in this function the HTML content is converted in to just text using a markup library `html2text` and later returned to the text_extraction function.

#### TEXT

```python
def information_extraction(text):
```

This is where all the information is extracted and most of the page ranking takes place. All keywords entered by the user are matched to extracted text using the NLP's name entity recognition making the process a lot faster than the regex script.

``` A glitch was encountered with the Spacy NER (Name Entity Recognizer), it accounts cryptocurrency values (bitcoin, stellar etc) as locations or person.``` 

Combo-Basic is also calculated on the text extracted, and is presented as key phrases in the final HTML report, `report.html`.

### Web Page Analyzer
This is the function which gets executed after the `final_1.json` has been produced. The HTML and CSS code are embeded in the python file  [webPageAnalyzer.py](webPageAnalyzer.py).
The CSS values of the webpage can be controlled using the function.

```python
def html_maker(content, path):
```

The final ouput of the program is `report.html`.

#### Screenshot
The screenshot function [screenshot.py](creenshot.py) is compatible with Firefox and Chrome webdrivers. It uses a headless browser and Selenium library, using the SOCKS5h proxy.
Any of the webdriver can be used by uncommenting the code in the function.

```python
def screenshot_taker(url, path):
```

`Call to Action`:
This is more of a to-do list, the NER rules for Spacy can be written more accurately avoiding a need for a trained model altogether.
There is a conflict between Pysocks library and Selenium, which is why a bash file was used to run all the programs, that need to fixed on a library level.
